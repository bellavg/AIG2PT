# @package _group_

# --- Training Configuration ---
# This file contains all settings related to the training process,
# including data, optimization, logging, and system settings.
#
# Server defaults (uncomment when running on GPU server):
# eval_interval: 1000
# log_interval: 10
# eval_iters: 200
# wandb_log: true
# wandb_project: 'AIG2PT'
# wandb_run_name: null
# batch_size: 64
# max_iters: 60000
# gradient_accumulation_steps: 10
# learning_rate: 0.00005
# min_lr: 0.00001
# backend: 'nccl'
# device: 'cuda'
# dtype: 'bfloat16'
# num_loader_workers: 8
# compile: false

# --- Data & I/O Settings ---
dataset: 'aig'
out_dir: 'results'
init_from: 'scratch' # Can be 'scratch', 'resume', or "gpt2-medium"

# --- Logging & Evaluation ---
eval_interval: 10
log_interval: 5
eval_iters: 5
always_save_checkpoint: false # If true, saves a checkpoint every eval_interval

# --- Weights & Biases Logging ---
wandb_log: false # Enable on server
wandb_project: 'AIG2PT'
wandb_run_name: null # Set on server (e.g., "aig2pt-base")

# --- Training Hyperparameters (local smoke defaults) ---
batch_size: 2
max_iters: 50
gradient_accumulation_steps: 1
patience: 7 # Early stopping patience (unused for smoke)

# --- Optimizer (AdamW) ---
learning_rate: 0.0003
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0

# --- Learning Rate Scheduler ---
decay_lr: true
warmup_iters: 0
lr_decay_iters: 50
min_lr: 0.00003

# --- System & DDP Settings ---
backend: 'gloo'
device: 'cpu'
dtype: 'float32'
compile: false
num_loader_workers: 0

